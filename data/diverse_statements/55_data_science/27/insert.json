[
  {
    "statements": [
      "INSERT INTO big_data_platforms (platform_id, platform_name, hadoop_support, spark_version) VALUES (1, 'Hadoop', TRUE, '2.4.0')",
      "INSERT INTO big_data_platforms (spark_version, hadoop_support, platform_name, platform_id) VALUES ('3.0.1', FALSE, 'Spark', 2)",
      "INSERT INTO big_data_platforms VALUES (3, 'MongoDB', FALSE, NULL)",
      "INSERT INTO big_data_platforms (platform_id, platform_name, spark_version, hadoop_support) VALUES (4, 'Kafka', '2.8.0', TRUE)",
      "INSERT INTO big_data_platforms (platform_name, hadoop_support, spark_version, platform_id) VALUES ('Databricks', TRUE, '3.1.0', 5)",
      "INSERT INTO big_data_platforms (platform_id, platform_name, hadoop_support, spark_version) VALUES (1, 'Hadoop Platform', TRUE, '3.1.2');",
      "INSERT INTO big_data_platforms (platform_id, platform_name, hadoop_support, spark_version) VALUES (2, 'Spark Platform', FALSE, '2.4.5');",
      "INSERT INTO big_data_platforms (platform_id, spark_version, hadoop_support, platform_name) VALUES (3, '3.0.0', TRUE, 'Presto Platform');",
      "INSERT INTO big_data_platforms (platform_name, hadoop_support, platform_id, spark_version) VALUES ('Flink Platform', FALSE, 4, '1.10.2');",
      "INSERT INTO big_data_platforms (platform_name, spark_version, hadoop_support, platform_id) VALUES ('Kudu Platform', '1.13.0', TRUE, 5);"
    ]
  },
  {
    "statements": [
      "INSERT INTO HadoopIntegration (id, name, age) VALUES (1, 'Alice', 30);",
      "INSERT INTO SparkIntegration (id, email, city) VALUES (1, 'alice@example.com', 'New York');",
      "INSERT INTO HadoopIntegration (id, name, age) VALUES (2, 'Bob', 25);",
      "INSERT INTO SparkIntegration (id, email, city) VALUES (2, 'bob@example.com', 'Los Angeles');",
      "INSERT INTO HadoopIntegration (name, id, age) VALUES ('Charlie', 3, 35);",
      "INSERT INTO HadoopIntegration (id, name, age) VALUES (1, 'Alice', 22);",
      "INSERT INTO SparkIntegration (id, city, email) VALUES (3, 'New York', 'alice@example.com');",
      "INSERT INTO HadoopIntegration (name, age, id) VALUES ('Bob', 28, 2);",
      "INSERT INTO SparkIntegration (city, email, id) VALUES ('San Francisco', 'bob@example.com', 4);",
      "INSERT INTO HadoopIntegration (name, age, id) VALUES ('Charlie', 25, 3);"
    ]
  },
  {
    "statements": [
      "INSERT INTO table1 (id, name, age) VALUES (1, 'John Doe', 30);",
      "INSERT INTO table2 (email, country, user_id) VALUES ('johndoe@example.com', 'USA', 1);",
      "INSERT INTO table3 (price, quantity, product_id) VALUES (100.00, 5, 101);",
      "INSERT INTO table1 (name, age, id) VALUES ('Alice Smith', 25, 2);",
      "INSERT INTO table2 (country, user_id, email) VALUES ('Canada', 2, 'alicesmith@example.com');",
      "INSERT INTO table1 (id, name, age) VALUES (1, 'Alice', 25);",
      "INSERT INTO table2 (email, user_id, country) VALUES ('alice@example.com', 1, 'USA');",
      "INSERT INTO table3 (price, product_id, quantity) VALUES (10.50, 101, 5);",
      "INSERT INTO table1 (name, age, id) VALUES ('Bob', 30, 2);",
      "INSERT INTO table2 (user_id, country, email) VALUES (2, 'Canada', 'bob@example.com');"
    ]
  },
  {
    "statements": [
      "INSERT INTO integration_hadoop_spark (id, name, age, location) VALUES (1, 'John Doe', 30, 'New York')",
      "INSERT INTO hadoop_data (data_id, data_name, data_value, data_type) VALUES (101, 'Data1', 500, 'text')",
      "INSERT INTO spark_analytics (worker_id, timestamp, analysis_result, record_id) VALUES (1, '2022-01-01 10:00:00', 'Success', 1001)",
      "INSERT INTO sql_bigdata_integration (end_time, job_id, job_name, start_time) VALUES ('2022-01-01 12:00:00', 201, 'Data Migration', '2022-01-01 10:00:00')",
      "INSERT INTO integration_hadoop_spark (id, name, age, location) VALUES (2, 'Jane Smith', 25, 'California')",
      "INSERT INTO hadoop_data (data_id, data_name, data_value, data_type) VALUES (1, 'data1', 100, 'type1')",
      "INSERT INTO spark_analytics (record_id, timestamp, analysis_result, worker_id) VALUES (1, '2022-01-01 10:00:00', 'Result A', 101)",
      "INSERT INTO sql_bigdata_integration (job_id, job_name, start_time, end_time) VALUES (1, 'Job A', '2022-01-01 09:00:00', '2022-01-01 10:00:00')",
      "INSERT INTO integration_hadoop_spark (id, name, age, location) VALUES (1, 'John Doe', 30, 'New York')",
      "INSERT INTO integration_hadoop_spark (id, name, age, location) VALUES (2, 'Jane Smith', 25, 'San Francisco')"
    ]
  },
  {
    "statements": [
      "INSERT INTO BigDataPlatforms(platform_id, platform_name) VALUES (1, 'Hadoop');",
      "INSERT INTO SQLDatabases(db_id, db_name, db_type) VALUES (1, 'Hive', 'Distributed');",
      "INSERT INTO Integration(integration_id, platform_id, db_id) VALUES (1, 1, 1);",
      "INSERT INTO HadoopTables(table_id, table_name, table_schema, db_id) VALUES (1, 'customers', 'customer_id INT, name VARCHAR(50)', 1);",
      "INSERT INTO SparkTables(table_id, table_name, table_schema, db_id) VALUES (1, 'orders', 'order_id INT, customer_id INT, amount DECIMAL(10, 2)', 1);",
      "INSERT INTO BigDataPlatforms (platform_id, platform_name) VALUES (1, 'Hadoop');",
      "INSERT INTO SQLDatabases (db_id, db_name, db_type) VALUES (1, 'Hive', 'Hadoop');",
      "INSERT INTO Integration (integration_id, platform_id, db_id) VALUES (1, 1, 1);",
      "INSERT INTO HadoopTables (table_id, table_name, table_schema, db_id) VALUES (1, 'employee', 'id INT, name STRING, salary DOUBLE', 1);",
      "INSERT INTO SparkTables (table_id, table_name, table_schema, db_id) VALUES (1, 'customer', 'id INT, name STRING, email STRING', 1);"
    ]
  }
]